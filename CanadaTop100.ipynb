{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaacce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c435b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_companies(page_url, output_csv=\"companies.csv\"):\n",
    "    \"\"\"\n",
    "    Uses Selenium to load `page_url`, finds all elements with class=\"company-info\",\n",
    "    and extracts:\n",
    "      1. Company Name  (from .company-header h3 a)\n",
    "      2. Company Link  (href of .company-header h3 a)\n",
    "      3. Location      (from .meta-entry.location .meta-entry-value)\n",
    "      4. Industry      (from .meta-entry.industry .meta-entry-value)\n",
    "      5. Employees     (from .meta-entry.employees .meta-entry-value)\n",
    "\n",
    "    Stores results into a pandas DataFrame and saves as CSV.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Set up Selenium WebDriver\n",
    "    driver = webdriver.Edge()\n",
    "\n",
    "    try:\n",
    "        # 2) Load the target page\n",
    "        driver.get(page_url)\n",
    "        # If the page needs time to load dynamically, you can wait a few seconds:\n",
    "        time.sleep(3)\n",
    "\n",
    "        # 3) Find all elements with class=\"company-info\"\n",
    "        company_elements = driver.find_elements(By.CLASS_NAME, \"company-info\")\n",
    "\n",
    "        # Prepare lists to collect each field\n",
    "        names = []\n",
    "        links = []\n",
    "        locations = []\n",
    "        industries = []\n",
    "        employees_list = []\n",
    "\n",
    "        # 4) Iterate over each company-info block\n",
    "        for idx, comp in enumerate(company_elements, start=1):\n",
    "            try:\n",
    "                # 4.1) Extract company header link (name + href)\n",
    "                header_a = comp.find_element(By.CSS_SELECTOR, \".company-header h3 a\")\n",
    "                company_name = header_a.text.strip()\n",
    "                company_link = header_a.get_attribute(\"href\").strip()\n",
    "            except NoSuchElementException:\n",
    "                company_name = \"\"\n",
    "                company_link = \"\"\n",
    "\n",
    "            # 4.2) Extract location\n",
    "            try:\n",
    "                loc_elem = comp.find_element(By.CSS_SELECTOR, \".meta-entry.location .meta-entry-value\")\n",
    "                location = loc_elem.text.strip()\n",
    "            except NoSuchElementException:\n",
    "                location = \"\"\n",
    "\n",
    "            # 4.3) Extract industry\n",
    "            try:\n",
    "                ind_elem = comp.find_element(By.CSS_SELECTOR, \".meta-entry.industry .meta-entry-value\")\n",
    "                industry = ind_elem.text.strip()\n",
    "            except NoSuchElementException:\n",
    "                industry = \"\"\n",
    "\n",
    "            # 4.4) Extract employees\n",
    "            try:\n",
    "                emp_elem = comp.find_element(By.CSS_SELECTOR, \".meta-entry.employees .meta-entry-value\")\n",
    "                employees = emp_elem.text.strip()\n",
    "            except NoSuchElementException:\n",
    "                employees = \"\"\n",
    "\n",
    "            # Append to lists\n",
    "            names.append(company_name)\n",
    "            links.append(company_link)\n",
    "            locations.append(location)\n",
    "            industries.append(industry)\n",
    "            employees_list.append(employees)\n",
    "\n",
    "        # 5) Build pandas DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            \"Company Name\": names,\n",
    "            \"Job Reviews URL\": links,\n",
    "            \"Location\": locations,\n",
    "            \"Industry\": industries,\n",
    "            \"Employees\": employees_list\n",
    "        })\n",
    "\n",
    "        # 6) Save to CSV\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"Scraped {len(df)} companies. Data saved to '{output_csv}'.\")\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b45aa551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 100 companies. Data saved to 'companies.csv'.\n"
     ]
    }
   ],
   "source": [
    "scrape_companies(\"https://www.canadastop100.com/national/\", output_csv=\"companies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc9761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
